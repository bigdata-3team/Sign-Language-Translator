{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = \"relu\", input_shape = (28, 28, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = tf.keras.optimizers.Adam()\n",
    "VALIDATION_SPLIT = 0.95\n",
    "\n",
    "IMG_ROWS, IMG_COLS = 28, 28\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "NB_CLASSES = 10  # NUMBER OF OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet(input_shape, classes):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(20, (5, 5), activation = \"relu\",\n",
    "                                    input_shape = input_shape))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "    # 더 깊은 계층에서 필터의 수를 늘림(딥러닝의 일반적 기술)\n",
    "    model.add(tf.keras.layers.Conv2D(50, (5, 5), activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(500, activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(classes, activation = \"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 20)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 50)          25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               400500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "X_train, X_test = X_train / 255, X_test / 255\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "model = LeNet(input_shape = INPUT_SHAPE, classes = NB_CLASSES)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = OPTIMIZER, metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 57000 samples\n",
      "Epoch 1/20\n",
      "3000/3000 [==============================] - 9s 3ms/sample - loss: 1.2295 - accuracy: 0.6810 - val_loss: 0.5117 - val_accuracy: 0.8241\n",
      "Epoch 2/20\n",
      "3000/3000 [==============================] - 1s 380us/sample - loss: 0.3566 - accuracy: 0.8943 - val_loss: 0.2987 - val_accuracy: 0.9108\n",
      "Epoch 3/20\n",
      "3000/3000 [==============================] - 1s 364us/sample - loss: 0.2277 - accuracy: 0.9363 - val_loss: 0.2246 - val_accuracy: 0.9305\n",
      "Epoch 4/20\n",
      "3000/3000 [==============================] - 1s 358us/sample - loss: 0.1620 - accuracy: 0.9533 - val_loss: 0.1972 - val_accuracy: 0.9391\n",
      "Epoch 5/20\n",
      "3000/3000 [==============================] - 1s 364us/sample - loss: 0.1315 - accuracy: 0.9650 - val_loss: 0.1791 - val_accuracy: 0.9438\n",
      "Epoch 6/20\n",
      "3000/3000 [==============================] - 1s 355us/sample - loss: 0.0980 - accuracy: 0.9750 - val_loss: 0.1686 - val_accuracy: 0.9478\n",
      "Epoch 7/20\n",
      "3000/3000 [==============================] - 1s 361us/sample - loss: 0.0790 - accuracy: 0.9773 - val_loss: 0.1353 - val_accuracy: 0.9578\n",
      "Epoch 8/20\n",
      "3000/3000 [==============================] - 1s 355us/sample - loss: 0.0582 - accuracy: 0.9843 - val_loss: 0.1205 - val_accuracy: 0.9634\n",
      "Epoch 9/20\n",
      "3000/3000 [==============================] - 1s 364us/sample - loss: 0.0474 - accuracy: 0.9873 - val_loss: 0.1242 - val_accuracy: 0.9611\n",
      "Epoch 10/20\n",
      "3000/3000 [==============================] - 1s 353us/sample - loss: 0.0351 - accuracy: 0.9920 - val_loss: 0.1309 - val_accuracy: 0.9596\n",
      "Epoch 11/20\n",
      "3000/3000 [==============================] - 1s 378us/sample - loss: 0.0301 - accuracy: 0.9920 - val_loss: 0.1229 - val_accuracy: 0.9620\n",
      "Epoch 12/20\n",
      "3000/3000 [==============================] - 1s 394us/sample - loss: 0.0234 - accuracy: 0.9940 - val_loss: 0.1193 - val_accuracy: 0.9644\n",
      "Epoch 13/20\n",
      "3000/3000 [==============================] - 1s 383us/sample - loss: 0.0179 - accuracy: 0.9960 - val_loss: 0.1140 - val_accuracy: 0.9661\n",
      "Epoch 14/20\n",
      "3000/3000 [==============================] - 1s 419us/sample - loss: 0.0118 - accuracy: 0.9987 - val_loss: 0.1096 - val_accuracy: 0.9680\n",
      "Epoch 15/20\n",
      "3000/3000 [==============================] - 1s 380us/sample - loss: 0.0115 - accuracy: 0.9980 - val_loss: 0.1361 - val_accuracy: 0.9615\n",
      "Epoch 16/20\n",
      "3000/3000 [==============================] - 1s 386us/sample - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.1133 - val_accuracy: 0.9681\n",
      "Epoch 17/20\n",
      "3000/3000 [==============================] - 1s 353us/sample - loss: 0.0053 - accuracy: 0.9997 - val_loss: 0.1292 - val_accuracy: 0.9658\n",
      "Epoch 18/20\n",
      "3000/3000 [==============================] - 1s 372us/sample - loss: 0.0052 - accuracy: 0.9997 - val_loss: 0.1205 - val_accuracy: 0.9681\n",
      "Epoch 19/20\n",
      "3000/3000 [==============================] - 1s 361us/sample - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.1194 - val_accuracy: 0.9686\n",
      "Epoch 20/20\n",
      "3000/3000 [==============================] - 1s 369us/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1185 - val_accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard()]\n",
    "history = model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS,\n",
    "                   verbose = VERBOSE, validation_split = VALIDATION_SPLIT, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 63us/sample - loss: 0.0922 - accuracy: 0.9729\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose = VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 IMAGE Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_CHANNELS = 3\n",
    "IMAGE_ROWS = 32; IMAGE_COLS = 32\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIMIZER = tf.keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(input_shape, classes):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation = \"relu\", input_shape = input_shape))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(512, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(classes, activation = \"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 53us/sample - loss: 0.0814 - accuracy: 0.9753 - val_loss: 0.0593 - val_accuracy: 0.9818\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0409 - accuracy: 0.9874 - val_loss: 0.0438 - val_accuracy: 0.9875\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0278 - accuracy: 0.9911 - val_loss: 0.0325 - val_accuracy: 0.9902\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.0411 - val_accuracy: 0.9882\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0457 - val_accuracy: 0.9878\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.0466 - val_accuracy: 0.9874\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.0473 - val_accuracy: 0.9889\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.0508 - val_accuracy: 0.9896\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0378 - val_accuracy: 0.9912\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0043 - accuracy: 0.9984 - val_loss: 0.0584 - val_accuracy: 0.9893\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0645 - val_accuracy: 0.9894\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0646 - val_accuracy: 0.9892\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0806 - val_accuracy: 0.9886\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0558 - val_accuracy: 0.9908\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0712 - val_accuracy: 0.9899\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0800 - val_accuracy: 0.9894\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 36us/sample - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0914 - val_accuracy: 0.9890\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 38us/sample - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.1022 - val_accuracy: 0.9888\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 37us/sample - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0820 - val_accuracy: 0.9902\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 35us/sample - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0840 - val_accuracy: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x201f01e7ec8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir = 'convnet')]\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = OPTIMIZER, metrics = [\"accuracy\"])\n",
    "model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS,\n",
    "          validation_split = VALIDATION_SPLIT, verbose = VERBOSE, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 20)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 50)          25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               400500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), padding = 'same',\n",
    "                                     input_shape = X_train.shape[1:], activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), padding = \"same\", activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding = \"same\", activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.BatchNormalizatino())\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding = 'same', activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(NUM_CLASSES, activation = \"softmax\"))\n",
    "    return model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "def load_data():\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    X_train = X_train.astype(\"float32\")\n",
    "    X_test = X_test.astype(\"float32\")\n",
    "    \n",
    "    mean = np.mean(X_tain, axis = (0, 1, 2, 3))\n",
    "    std = np.std(X_train, axis = (0, 1, 2, 3))\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_test = (X_test - mean) / std\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      " 15343616/170498071 [=>............................] - ETA: 1:28:42"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-73866942eaf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m model.compile(optimizer = \"RMSprop\", loss = \"cateogrical_crossentropy\",\n\u001b[0;32m      4\u001b[0m              metrics = [\"accuracy\"])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-e94ddb63e0a5>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcifar10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\cifar10.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m       \u001b[0muntar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m       \u001b[0mfile_hash\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m       '6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce')\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mnum_train_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sudal\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "model = build_model()\n",
    "model.compile(optimizer = \"RMSprop\", loss = \"cateogrical_crossentropy\",\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "batch_size = 64\n",
    "model.fit(X_train, y_train, batch_size = batch_size, epochs = EPOCHS,\n",
    "          validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 보강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range = 30, width_shift_range = 0.2,\n",
    "                            height_shift_range = 0.2, horizontal_flip = True,)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                   epochs = EPOCHS, verbose = 1, validation_split = 0.2)\n",
    "\n",
    "# 디스크에 저장\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10으로 예측\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from tf.keras.models import model_from_json\n",
    "from tf.keras.optimizer import SGD\n",
    "\n",
    "# 모델 로드\n",
    "model_architecture = \"cifar_architecture.json\"\n",
    "model_weights = 'cifar10_weights.h5'\n",
    "model = model_from_json(open(model_architecture).read())\n",
    "model.load_weights(model_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "def VGG1_16(weights_path = None):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.ZeroPadding2D((1, 1), input_shape = (224, 224, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "    model.add(layers.ZeroPadding2D((1, 1)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    model.add(layers.ZeroPadding2D((1, 1)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation = \"relu\"))\n",
    "    \n",
    "    model.add(layers.ZeroPadding2D((1, 1)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), actvation = \"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    model.add(layers.ZeroPadding2D((1, 1)))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation = \"relu\"))\n",
    "    model.add(layers.ZeroPadding2D((1, 1)))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation = 'relu'))\n",
    "    \n",
    "    model.add(layers.ZeroPadding2D((1, 1)))\n",
    "    model.add(layers.Conv2D())\n",
    "    \n",
    "    if weights_path:\n",
    "        model.load_weights(weigths_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특징 추출을 위해 사전 구축된 딥러닝 모델 재활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cf9e0405d007>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "base_model = VGG16(weights = 'imagenet', include_top = True)\n",
    "print(base_model)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name, layer.output_shape)\n",
    "\n",
    "# block4_pool 블록에서 특징 추출\n",
    "model = models.Model(inputs = base_model.input,\n",
    "                     outputs = base_model.get_layer('block4_pool').output)\n",
    "\n",
    "img_path = \"cat.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.3.0.36-cp37-cp37m-win_amd64.whl (33.4 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\shdhk\\anaconda3\\envs\\sudal\\lib\\site-packages (from opencv-python) (1.18.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.3.0.36\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
